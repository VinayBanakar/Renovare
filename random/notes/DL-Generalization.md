### [UNDERSTANDING DEEP LEARNING REQUIRES RETHINKING GENERALIZATION](https://arxiv.org/pdf/1611.03530.pdf)

With large neural networks test accuracy being increasingly taken for granted, Chiyuan et al’s seminal work on debunking strongly held believes about generalization acts as a nudge for the ML community to take a deeper look into the need and effects of rudimentary techniques used during training. The authors in an attempt to understand what distinguishes a neural network that generalizes well from those that don’t, introduce randomization tests. In this quest they identify two major unforeseen insights on effective capacity of neural networks and implications of regularization on generalization. Also they prove two theorems, one extends the universal approximation theorem to two layers with ReLUs and the other shows that SGD does implicit regularization that results in convergence to a least norm solution.  

The main need for new tests was because traditional measures like counting the number of parameters or VC dimensions struggle to explain the effective complexity. Randomization tests are simply a set of experiments designed to fit random noise in data which contains no relationships so that the effective capacity of the model can be measured. They compare AlexNet, Inception and MLP on different input datasets which range from corrupted labels, shuffled pixels, to Gaussian noise. In which they identify that random labels still fit perfectly in training set which is surprising because by randomizing we completely destroy the relation between image and the label. Also, their tests indicate network choice seems to make a big influence on generalization error, nevertheless they all seem to converge on test accuracy as corruption increases. To conclude, DNNs can easily fit random labels. Which implies, the effective capacity is sufficient for memorizing the entire data set and the optimization on random labels remains easy. From early convention it was assumed that the ease of optimization was because the network is learning abstractions between classes and if it had to memorize every input then it would take much longer to converge but this has proven to be wrong.  

They determine that there is no significant difference in generalization performance between using regularization and not. So to improve generalization, it alone is neither necessary nor sufficient. However, model architecture seems to be playing a much more important role in generalization than the regularization techniques. Finally, the authors conclude that regularizers are not the fundamental reason for generalization. This is an iconoclastic discovery because hitherto it was believed that when networks had to fit an enormous input space, the only way of guaranteeing good generalizing performance was to use these regularization techniques which limited the possible search space.  

This begs the question of whether neural networks are just good memorizers? [Devansh et al](https://arxiv.org/pdf/1706.05394.pdf) show it might be not, as on random dataset the cost function linearly decreases with number of epochs (struggling to find the local minima of right weights to get the accuracy) whereas on structured dataset it was easier to find local maxima. Additionally, [Anna et al](https://arxiv.org/pdf/1412.0233.pdf) also showed that with structured data it is easier to find the local minima.