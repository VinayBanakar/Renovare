## SSD structure

**V-NAND/3D-NAND memory cells:**  
Basic unit is a Charge Trap Flash memory cell. Each cell stores information by placing different levels of electrons on to a charge trap. Previously, one cell could only store two different level of electrons (1 or 0 bit) but most memory cells today can hold 8-16 different levels, now a single cell can store (3 [TLC] or 4 [QLC] bits). Charge traps can hold the electrons/state for a long time (to see how writes/reads or erase is done see [1]). These memory cells are stacked Vertically, hence V Nand is called a String. Typically 96-136 CTFs form a String. When information is read from or to, only once cell can be activated in the String and to do that there are separate control gates attached to every layer in the String. When control gates are activated to read a cell, it sends to the Bitline that resides on top of the String via channel formed by the vertical stack. Same sequence happens when charges are added to charge trap via channel, which is how writes are done. At any given time only 1 layer is read from/written to in a String. Multiple Strings (usually 30k-60k) are placed next to each other on common control gates, these are called Row. CTFs on a single control gate is called a Page, which means every time a control gate is activated all the CTFs on it are read from/written to at the same time as they are in the same layer. Bitlines on the top of strings are perpendicular to control gates. Next, 4-8 consecutive rows form a single block. The bitlines (columns) on top of each String in a block are connected together, so they all share the same bitline. Bitline selectors, which are control gates ensures only one row in using the bitline at a time (effectively ensuring only one page can be read from a block at a time). In total there are about 4k-6k blocks per chip and two chips usually make a chiplet/die. The control gate selectors and bitline selectors are on the opposite side, together they comprise a Row decoder,  which allows us to access a single page. All the bitlines are connected to a page buffer, where the information of a single page is read from/written to at a time. All this are at a nanoscopic (few hundred atoms wide) scale. Typically 8-16 chiplets/dies are stacked together to form a 3D integrated chip (this package is called 3D die stacking, another ex: RAM stacked on CPU in a SoC), which operate in parallel.

_Information is stored as pages. When SSD reads/writes information it does it in pages. But when computer erases information, it erases it in blocks. This is because the erasing process requires around 25-30 V compared to reading and writing process around 2-7V. Also, when data is erased, the entire block is erased, so sometimes when you want to erase just a page the surrounding pages are moved elsewhere, and the entire block is erased. This is because the act of erasing requires high amounts of voltage, which could cause errors in neighboring pages when done at page level._

Latency order in SSDs: Read (25 [SLC] -100 [TLC] us) < Write (250 [SLC] -1500 [TLC] us) < Erase (1500 [SLC] -5000 [TLC] us)
Typical triple-level-cell (TLC) latency is 4x worse compared with single-level cell (SLC) NAND for reads, but 6x worse for writes. Erase latencies are also significantly impacted. The impact isn’t proportional, TLC NAND is nearly twice as slow as MLC NAND, despite holding just 50% more data (three bits per cell, instead of two). Reading the proper value out of the cell requires the memory controller to use a precise voltage to ascertain whether any particular cell is charged, this contributes to the latency discrepancy.
https://www.extremetech.com/extreme/210492-extremetech-explains-how-do-ssds-work   

The only way for an SSD to update an existing page is to copy the contents of the entire block into memory, erase the block, and then write the contents of the old block + the updated page. If the drive is full and there are no empty pages available, the SSD must first scan for blocks that are marked for deletion but that haven’t been deleted yet, erase them, and then write the data to the now-erased page. This is why SSDs can become slower as they age — a mostly-empty drive is full of blocks that can be written immediately, a mostly-full drive is more likely to be forced through the entire program/erase sequence. The only way for an SSD to update an existing page is to copy the contents of the entire block into memory, erase the block, and then write the contents of the old block + the updated page. If the drive is full and there are no empty pages available, the SSD must first scan for blocks that are marked for deletion but that haven’t been deleted yet, erase them, and then write the data to the now-erased page. This is why SSDs can become slower as they age — a mostly-empty drive is full of blocks that can be written immediately, a mostly-full drive is more likely to be forced through the entire program/erase sequence. Garbage collection: erasing blocks like above is done in the background during idle. _A less important reason for blocks to be moved is the read disturb. Reading can change the state of nearby cells, thus blocks need to be moved around after a certain number of reads have been reached._  A way to improve performance in SSDs is to split **cold** and **hot** data (in same page) as much as possible into separate pages, which will make the job of the garbage collector easier but wear levelling ensures blocks containing hot and cold data are frequently swapped. Extremely hot data should be buffered as much as possible and written to the drive as infrequently as possible.

http://codecapsule.com/2014/02/12/coding-for-ssds-part-3-pages-blocks-and-the-flash-translation-layer/  
Never write less than a page, align writes on the page size, and write chunks of data that are multiple of the page size. To maximize throughput, whenever possible keep small writes into a buffer in RAM and when the buffer is full, perform a single large write to batch all the small writes.

Wear leveling refers to the practice of ensuring certain NAND blocks aren’t written and erased more often than others. While wear leveling increases a drive’s life expectancy and endurance by writing to the NAND equally, it can actually increase write amplification. In other to distribute writes evenly across the disk, it’s sometimes necessary to program and erase blocks even though their contents haven’t actually changed. A good wear leveling algorithm seeks to balance these impacts. **Block management is a trade-off between maximizing wear leveling and minimizing write amplification.**

Conventional SSDs attached via SATA top out at ~550MB/s in terms of practical read/write speeds. M.2 drives are capable of substantially faster performance into the 3.2GB/s range.  

**Flash Translation Layer (FTL)**, resides in the SSD controller. The FTL is critical and has two main purposes: logical block mapping and garbage collection. Array of Logical Block Addresses (LBA) makes sense for HDDs as their sectors can be overwritten, but it is not fully suited to the way flash memory works. Hence, a logical block mapping is required. The most common is the log-block mapping, which works in a way that is similar to log-structured file systems. This allows random writes to be handled like sequential writes. http://pages.cs.wisc.edu/~remzi/OSTEP/file-ssd.pdf and http://codecapsule.com/wp-content/uploads/2014/02/ssd-hybrid-ftl.jpg for log merge strategy.

**Over-provisioning** is simply having more physical blocks than logical blocks, by keeping a ratio of the physical blocks reserved for the controller and not visible to the user. Most manufacturers of professional SSDs already include some over-provisioning, generally in the order of 7 to 25%. Performance increased dramatically simply by making sure that 25% of the space was reserved for over-provisioning — [summing up all levels of over-provisioning](https://www.anandtech.com/show/6489/playing-with-op).Over-provisioning can act as a buffer to absorb high throughput write workloads, leaving enough time to the garbage collection to catch up and erase blocks again and not erase in the write path.

**Parallel read/writes:** Most SSDs are built on an array of flash memory packages, which are connected through multiple channels to flash memory controllers. SSDs provide logical block addresses (LBA) as a logical interface to the host. Since logical blocks can be striped over multiple flash memory packages (called clustered blocks), data accesses can be conducted independently in parallel. Such a highly parallelized design yields two benefits: (1) Transferring data from/to multiple flash memory packages in parallel can provide high bandwidth in aggregate. (2) High latency operations can be effectively hidden behind other concurrent operations.  
Random writes are as fast as sequential writes if writes are cluster block aligned, if writes are smaller then cluster block size then random writes are slower than sequential writes.
>>  Indeed, some SSDs are using tree-like data structures to represent the mapping between logical block addresses and physical block addresses [here](http://web.cse.ohio-state.edu/hpcs/WWW/HTML/publications/papers/TR-09-2.pdf), and a lot of small random writes will translate into a lot of updates to the mapping in RAM. As this mapping is being persisted from RAM to flash memory [ref](https://www.microsoft.com/en-us/research/wp-content/uploads/2008/06/USENIX-08-SSD.pdf), all those updates in the RAM will cause a lot of writes on the flash memory. A sequential workload incurs less updates to the metadata, and therefore less writes to the flash memory.  Another reason is that if the random writes are small, they will cause a higher number of copy-erase-write operations on the blocks. On the other hand, sequential writes of at least the size of a block allow for the faster switch merge optimization to be used. Moreover, small random writes are known to invalidate data randomly. Instead of having a few blocks fully invalidated, many blocks will have only one page invalidated, which causes stale pages to be spread out in physical space instead of being localized. This phenomenon is known as **internal fragmentation**, and causes the cleaning efficiency to drop, by requiring a larger number of erase operations to be run by the garbage collection process to create free pages.  
>> **A single large write is better than many small concurrent writes**: A single large write request offers the same throughput as many small concurrent writes, however in terms of latency, a large single write has a better response time than concurrent writes. Therefore, whenever possible, it is best to perform large writes. **When the writes are small and cannot be grouped or buffered, multi-threading is beneficial**: Many concurrent small write requests will offer a better throughput than a single small write request. So if the I/O is small and cannot be batched, it is better to use multiple threads.  
http://csl.skku.edu/papers/CS-TR-2010-329.pdf  


>> **To improve the read performance, write related data together:** Read performance is a consequence of the write pattern. When a large chunk of data is written at once, it is spread across separate NAND-flash chips. Thus you should write related data in the same page, block, or clustered block, so it can later be read faster with a single I/O request, by taking advantage of the internal parallelism.  
**A single large read is better than many small concurrent reads** Concurrent random reads cannot fully make use of the readahead mechanism. In addition, multiple Logical Block Addresses may end up on the same chip, not taking advantage or of the internal parallelism. Moreover, a large read operation will access sequential addresses and will therefore be able to use the readahead buffer if present. Consequently, it is preferable to issue large read requests.  
>> **Separate read and write requests** A workload made of a mix of small interleaved reads and writes will prevent the internal caching and readahead mechanism to work properly, and will cause the throughput to drop. It is best to avoid simultaneous reads and writes, and perform them one after the other in large chunks, preferably of the size of the clustered block. For example, if 1000 files have to be updated, you could iterate over the files, doing a read and write on a file and then moving to the next file, but that would be slow. It would be better to reads all 1000 files at once and then write back to those 1000 files at once.

**_A write that is not aligned to page size will end up becoming read-modify-write operation._**


A quick summary: http://codecapsule.com/2014/02/12/coding-for-ssds-part-6-a-summary-what-every-programmer-should-know-about-solid-state-drives/
[1] ./Quantum-Tunneling-SSDs.md

***


**F2FS:** https://www.usenix.org/system/files/conference/fast15/fast15-paper-lee.pdf & https://www.kernel.org/doc/html/v5.7/filesystems/f2fs.html  

btrfs is a flash friendly file system -  
https://www.reddit.com/r/btrfs/comments/ckqt51/how_flashfriendly_is_btrfs/


[Accelerating RocksDB with Zoned namespaces](https://www.youtube.com/watch?v=FAvoZ2rHzjA) https://imgur.com/a/gWnTdUH  
Smart data placement: Only ~1X write amplifications (compared to 3-6X)  
No on-drive garbage collection: 20% increase in capacity and 20% TCO reduction (increases lifetime/writes significantly). Possibly 4x performance throughput increase since 4x lower write amplifications compared to traditional SSDs.


Optane SSD performance:  
https://www.usenix.org/system/files/hotstorage19-paper-wu-kan.pdf  

>> Unless transfer rates grow much faster than storage rates, FedEx will always be faster to send large amounts of data than internet, but this seems unlikely, since storage and transfer are fundamentally linked. High throughput but low latency https://what-if.xkcd.com/31/